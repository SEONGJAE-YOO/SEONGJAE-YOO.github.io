<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>머신러닝 프로그래밍 10주차 - Regression Gradient Descent</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- 웹폰트 추가 -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- syntax.css 추가 -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />



    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="SeongJae Yu 블로그" />
<link rel="shortcut icon" href="https://SEONGJAE-YOO.github.io/assets/built/images/favicon.jpg" type="image/png" />
<link rel="canonical" href="https://SEONGJAE-YOO.github.io/10" />
<meta name="referrer" content="no-referrer-when-downgrade" />

 <!--title below is coming from _includes/dynamic_title-->
<meta property="og:site_name" content="Big Data" />
<meta property="og:type" content="website" />
<meta property="og:title" content="머신러닝 프로그래밍 10주차 - Regression Gradient Descent" />
<meta property="og:description" content="SeongJae Yu 블로그" />
<meta property="og:url" content="https://SEONGJAE-YOO.github.io/10" />
<meta property="og:image" content="https://SEONGJAE-YOO.github.io/assets/built/images/logo-python.png" />
<meta property="article:publisher" content="https://www.facebook.com/" />
<meta property="article:tag" content="Python" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="머신러닝 프로그래밍 10주차 - Regression Gradient Descent" />
<meta name="twitter:description" content="SeongJae Yu 블로그" />
<meta name="twitter:url" content="https://SEONGJAE-YOO.github.io/" />
<meta name="twitter:image" content="https://SEONGJAE-YOO.github.io/assets/built/images/logo-python.png" />
<meta name="twitter:label1" content="Written by" />
<meta name="twitter:data1" content="Big Data" />
<meta name="twitter:label2" content="Filed under" />
<meta name="twitter:data2" content="Python" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@" />
<meta property="og:image:width" content="2000" />
<meta property="og:image:height" content="666" />

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Website",
        "publisher": {
            "@type": "Organization",
            "name": "Big Data",
            "logo": "https://SEONGJAE-YOO.github.io/"
        },
        "url": "https://SEONGJAE-YOO.github.io/10",
        "image": {
            "@type": "ImageObject",
            "url": "https://SEONGJAE-YOO.github.io/assets/built/images/logo-python.png",
            "width": 2000,
            "height": 666
        },
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://SEONGJAE-YOO.github.io/10"
        },
        "description": "SeongJae Yu 블로그"
    }
</script>

<!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
<script type="text/javascript">
ghost.init({
    clientId: "ghost-frontend",
    clientSecret: "f84a07a72b17"
});
</script> -->

<meta name="generator" content="Jekyll 3.6.2" />
<link rel="alternate" type="application/rss+xml" title="머신러닝 프로그래밍 10주차 - Regression Gradient Descent" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://SEONGJAE-YOO.github.io/">Big Data</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="https://codingstorylove.github.io/about/#page-title">About blog</a></li>
    <li class="nav-rprogramming" role="menuitem"><a href="/tag/rprogramming/">R</a></li>
    <li class="nav-processmining" role="menuitem"><a href="/tag/processmining/">Processmining</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/python/">Python</a></li>
    <li class="nav-Algorithm" role="menuitem"><a href="https://codingstorylove.github.io/">Algorithm</a></li>
    <li class="nav-kaggle" role="menuitem"><a href="/tag/kaggle">kaggle</a></li>
    <li class="nav-book" role="menuitem"><a href="/tag/book">book</a></li>
    <li class="nav-udemy" role="menuitem"><a href="/tag/udemy">udemy</a></li>
    <li class="nav-thesis" role="menuitem"><a href="/tag/thesis">thesis</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tag별 Posts</a>
    </li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-python post tag-python ">

            <header class="post-full-header">
                <h1 class="post-full-title">머신러닝 프로그래밍 10주차 - Regression Gradient Descent</h1>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/built/images/logo-python.png)">
            </figure>
            

            <section class="post-full-content">
                <h1 id="regression-gradient-descent">Regression Gradient Descent</h1>

<h1 id="선형-회귀linear-regression">선형 회귀(Linear Regression)</h1>

<h1 id="참고사이트----httpswikidocsnet21670">참고사이트  - <a href="https://wikidocs.net/21670">https://wikidocs.net/21670</a></h1>

<h1 id="1-단순-선형-회귀-분석simple-linear-regression-analysis">1) 단순 선형 회귀 분석(Simple Linear Regression Analysis)</h1>

<h1 id="2-다중-선형-회귀-분석multiple-linear-regression-analysis">2) 다중 선형 회귀 분석(Multiple Linear Regression Analysis)</h1>

<h1 id="비용-함수cost-function--평균-제곱-오차mse">비용 함수(Cost function) : 평균 제곱 오차(MSE)</h1>

<p>앞서 주어진 데이터에서 와 의 관계를 와 를 이용하여 식을 세우는 일을 가설이라고 언급했습니다. 그리고 이제 해야할 일은 문제에 대한 규칙을 가장 잘 표현하는 와 를 찾는 일입니다. 머신 러닝은 와 를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 와 를 찾아냅니다.</p>

<p>이 때 실제값과 예측값에 대한 오차에 대한 식을 목적 함수(Objective function) 또는 비용 함수(Cost function) 또는 손실 함수(Loss function)라고 합니다. 함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수를 목적 함수(Objective function)라고 합니다. 그리고 값을 최소화하려고 하면 이를 비용 함수(Cost function) 또는 손실 함수(Loss function)라고 합니다. 이 책에서는 목적 함수, 비용 함수, 손실 함수란 용어를 같은 의미로 혼용해서 사용합니다.</p>

<p>비용 함수는 단순히 실제값과 예측값에 대한 오차를 표현하면 되는 것이 아니라, 예측값의 오차를 줄이는 일에 최적화 된 식이어야 합니다. 앞으로 배울 러닝, 딥 러닝에는 다양한 문제들이 있고, 각 문제들에는 적합한 비용 함수들이 있습니다. 회귀 문제의 경우에는 주로 평균 제곱 오차(Mean Squared Error, MSE)가 사용됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">1.6</span>
<span class="n">c</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span>
</code></pre></div></div>

<h1 id="numpyvectorize이란">numpy.vectorize이란?</h1>

<h1 id="참고사이트---httpsnumpyorgdocstablereferencegeneratednumpyvectorizehtml">참고사이트 - <a href="https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html">https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html</a></h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>  <span class="c1"># 주피터 노트북에 이미지 삽입
</span><span class="n">Image</span><span class="p">(</span><span class="s">"C://Users/MyCom/jupyter-tutorial/수업자료/data/20211117_003551_1.png"</span><span class="p">)</span>

</code></pre></div></div>

<p><img src="./img/machinelearning/10/output_6_0.png" alt="output_6_0" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">):</span>
    <span class="c1">#return a*x*x + b*x + c #2차함수 형태
</span>    <span class="k">return</span> <span class="n">a</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
<span class="c1"># def f(x, a, b):  #1차함수형태
#     return a*x + b
</span>    <span class="c1">#return math.sin(a*x+b)
# vf = np.vectorize(f)
</span></code></pre></div></div>

<p>-uniform() 함수</p>

<p>2개의 숫자 사이의 랜덤 실수를 리턴합니다.</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>random.uniform(1, 10)         # Random float x, 1.0 &lt;= x &lt; 10.0</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <p>결과 값 : 1.1800146073117523</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>  <span class="c1">#randint는 정수를 uniform은 소수를 반환한다
# ys = f(xs,a,b,c) + np.random.normal(0,10,n) # 1차함수
</span><span class="n">ys</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>    <span class="c1">#noise 추가 / normal (평균, 표준편차,만들어질 갯수) # 참고사이트 - https://lngnat.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC-Normal-Distribution-NumPyrandomnormal%ED%8F%89%EA%B7%A0%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8%EA%B0%AF%EC%88%98
#2차함수
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xs</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([-8.82450096,  8.35626007,  8.48698725,  1.41962675, -1.06510406,
       -5.07269139, -8.48833348, -2.11441152,  1.46876428, -2.93171198,
       -5.78185912,  9.01150006,  1.9069546 , -4.04589237, -5.92454961,
        0.21274725,  4.38478928, -5.42623489, -6.29912032, -6.63538446,
        8.84752356, -1.88496897,  7.97902809, -3.57310765, -3.56124911,
        4.1458015 , -6.13943846, -9.78998499,  9.45614955,  1.78961294,
       -9.91266107,  9.37651054, -4.65389185, -5.15437226,  7.09064049,
       -3.72317394,  4.19433237,  1.43072925, -7.1189374 ,  8.99745117,
       -9.88745586,  3.01156743, -3.31321318, -6.99529032, -4.04959163,
       -6.75505026, -0.26651557,  1.52938598, -7.05729805, -2.15895518,
       -8.32556604,  1.51520421, -7.25254632, -9.85233047, -5.65318472,
        5.62698891, -5.51617032, -0.39112499, -0.33194447,  3.5642697 ,
        3.76209268, -7.41973909, -2.83445195, -5.73555473, -2.63190555,
       -0.52137738, -9.30054634, -8.10941359, -6.53975374, -0.20269328,
        0.5145176 ,  7.24360591,  5.99681504,  6.6239364 ,  6.80770691,
       -1.39463753, -5.52092595, -6.53239334, -0.12429043, -5.71684258,
       -1.41836688, -8.2535425 ,  3.175049  , -4.05423072, -0.10799905,
       -4.25134831,  8.85242881,  9.96253069, -3.3367241 , -1.4493517 ,
        1.55906117, -8.46887337, -8.93214856, -0.45819726, -8.39542314,
        7.1332123 ,  3.88324879, -4.22276731, -8.38518956, -4.86069379,
        3.62265617,  8.95106384,  9.66874208,  0.16319792, -7.81746697,
       -7.99780849, -8.66880192, -7.88604183, -6.20942053, -7.16168191,
        4.76628932, -0.08604518, -7.52072956, -7.61414723,  8.91029293,
       -4.26513484,  0.3837306 , -6.79427942,  9.49125564,  5.56347336,
       -5.43190082, -0.15607811,  2.99897718, -3.50404086, -6.02506022,
       -6.08771578,  2.01412929, -4.8760307 ,  3.77947048, -4.25593801,
        6.18855131,  3.37448288,  8.1712301 ,  7.38822221,  3.78341934,
        2.07014865,  9.46500589, -2.97791217, -2.56721005, -0.0656148 ,
       -3.77247349, -4.9111143 , -2.13403781,  6.57597064,  1.67505357,
        3.69998754, -7.51783802, -6.04484207,  0.84290602,  9.72284651,
       -2.51509898, -6.6551389 , -4.58099345, -7.5797443 ,  4.17838948,
       -3.12111814, -7.76455697,  9.59514807,  6.85949525, -4.15070149,
       -6.10254394,  7.21138513, -4.76004024, -1.74162444, -3.14739487,
       -2.92748158, -4.70850376, -3.64457411,  4.246319  ,  7.72470543,
        4.59270536, -3.8285139 , -5.63776405,  7.32020783, -9.02382367,
        2.94915453, -1.0634599 ,  6.35224358,  1.11102993,  9.15862349,
        5.78833701, -6.02209631,  8.35548943, -2.15594363,  1.52398245,
        0.84672962, -1.62643178,  1.1787552 , -3.1653484 , -9.95581726,
       -8.4148917 ,  7.8145782 , -9.0598313 ,  5.24677638, -1.31823573,
       -1.89289641,  1.94626889,  2.44795796,  8.58059276, -7.12080131,
       -9.32126322,  5.03403923,  3.71904243,  7.11219699,  7.87645983,
       -5.68800647, -1.50584184, -9.20346735, -8.25003537,  7.67051917,
       -3.36181384, -3.93995486,  1.91927534,  0.7850455 ,  2.10639639,
        4.98924884, -4.94013614, -3.90117475,  0.94380741,  0.10395165,
        0.4626688 ,  1.10594747, -8.38912759,  8.792229  , -3.88434184,
       -3.25004609, -1.92264504, -2.27935075,  3.58199062,  3.6592385 ,
        0.8257611 ,  7.2106722 , -5.8673145 , -7.3294766 , -7.13812962,
        0.47443151, -8.45667227,  7.62723939,  3.16300657, -7.39776645,
        2.90154736,  0.11259181,  8.05883445,  3.68919492,  4.4235687 ,
        7.97156418, -0.85523058, -5.84223877, -8.74297601, -5.30229967,
        5.12037168,  9.02946969, -2.49330399,  5.14949742, -8.26703426,
        7.0148967 , -8.41439233,  7.11268464,  1.97820582, -5.91192613,
        8.57509742, -7.40607138,  3.47231959,  7.85263074, -0.16774354,
       -8.94299044,  4.44842666,  5.24395354,  2.42608216, -0.42221399,
        3.79364468,  3.20839214,  2.83929874,  7.33981982, -6.03290378,
       -4.80721672, -1.3173894 , -5.10360516, -4.09208608,  9.04309994,
       -5.6648452 , -9.91521075,  4.52944444,  7.77701095, -6.35827256,
       -8.38051222,  3.24720986, -3.91040484,  5.00749974, -9.78900477,
        3.52590552, -1.03270246, -4.14761154, -0.10066622, -9.01996868,
       -6.19235415, -2.41134607,  5.41132572, -1.53208389,  0.74226056,
       -4.84605165, -3.27412518,  3.12167179, -4.84657059,  5.15630954,
        5.06465315,  4.95009655,  5.43626962, -6.57196957, -8.19316819,
       -3.01753242,  0.40885492, -5.99804447,  4.93059836, -0.95912138,
       -9.35977347, -8.23316225, -0.28397579,  8.43844063, -4.3135255 ,
       -2.80746955,  4.02984491, -2.02261723, -7.93024724,  0.58573894,
        9.17642626,  3.52120209, -6.51846145, -3.52389109,  8.46796388,
       -7.92618622, -5.88917369,  7.36445309, -2.09083814, -0.28082232,
        4.77284202, -8.46202707, -4.31872913, -6.3175906 , -7.68735731,
       -3.17718596, -3.80407262, -0.24137666,  7.3121785 , -3.05038644,
        9.62804267,  9.56800596,  8.78322082,  9.56006026, -1.96892445,
        3.99579167,  2.95438539, -5.47989802, -5.68473639,  5.64156555,
       -1.75920524,  6.26321723,  0.26425384, -1.49778286,  9.92540513,
       -7.34110554,  2.23725519, -2.82310168, -3.8648367 ,  8.42501897,
       -2.70664492, -4.91193181,  0.25625718, -1.16848806,  1.22443867,
       -9.98343735, -5.59774943, -9.37867496,  4.031186  , -3.47216747,
       -0.56448791, -3.54534904, -0.23240101, -4.8725857 , -9.14379381,
       -2.50783462, -8.28429973, -7.49165875,  8.36193742,  7.82438516,
       -6.62141959, -0.6348411 , -0.34349315,  0.20840962,  9.39210552,
       -9.32018427, -6.44820774,  2.43508308,  5.88763925,  5.33959252,
       -0.98341209,  8.0840926 , -1.70980916, -4.2074682 ,  0.45220001,
       -1.15785262,  9.85293816, -7.52516425,  8.94382877,  0.20500227,
       -4.03812843, -8.94069514,  5.56622595, -0.4814476 , -8.43441478,
        1.0231455 , -6.72941568,  0.92474421, -5.92929824,  5.66423663,
        1.20764531,  0.40371695,  6.21224496,  4.88388099, -3.97300848,
        9.56332116,  3.15629847,  5.74061535, -7.43781357,  2.56937527,
       -5.12127996,  2.14308116,  7.38264169, -7.16603693, -2.67634218,
        6.6890594 ,  1.63603528, -2.31545244,  3.59738852, -7.8381993 ,
       -6.03438108,  3.19760919,  0.13117816,  8.35526352,  0.06192172,
       -1.67858187, -4.89934542, -2.19386041,  3.15934698,  3.05127875,
       -5.52766433, -2.44789547,  6.19323293, -1.29220432,  2.55410193,
        3.41950739,  9.12552716,  4.58089419,  9.5162472 , -3.97705633,
       -0.60644106, -2.4946234 , -6.46175527,  6.69733804,  4.58749378,
        9.0302445 , -2.15743602, -2.1320435 ,  6.4270277 ,  5.68214905,
       -6.81073651,  8.36368183,  7.34810634, -7.07907175,  8.62042131,
       -2.43769107,  5.87279417, -7.45361783,  5.06985699,  9.54927133,
       -8.99883367,  9.01543793,  2.04475522,  8.72202199,  5.46322901,
       -5.12836184,  8.65831771, -4.48646683,  6.01237451, -3.30848378,
       -8.41683157, -5.4760174 ,  8.39289351, -1.88054622,  1.68404438,
        1.4287122 ,  2.0535045 , -6.39740138, -0.6576019 ,  7.19749769,
       -0.14512938,  3.28160302,  2.20698896,  0.89480673,  3.85779572,
       -2.09392987,  2.80888012,  6.10379094,  8.115145  ,  2.97472402,
        7.75592919, -9.57135587,  0.28060373, -7.94443901,  6.39706711,
       -0.44983579,  4.3833689 ,  9.21618502, -8.63969568,  2.625887  ,
       -5.07757567, -5.74116246, -9.25337365, -7.65799007,  7.16584356,
       -1.45678855, -9.64055305,  2.28386869, -0.60004658, -9.98861755,
       -9.75868336,  1.19202758,  5.10853877,  3.90426036, -2.91400426,
       -8.34639662, -2.74549632, -8.28052859, -1.67928943, -9.32057149,
       -1.99378113, -1.6782252 ,  0.05269223,  8.48519948, -0.97993055,
        2.3409084 , -4.32805102, -2.01456102, -6.81020437, -8.81674496,
        4.55189956, -1.17573182,  6.57244644,  7.60402957,  8.02918828,
        2.62678018, -2.84726538, -4.2237969 , -0.85356231,  2.55835208,
        0.88620787,  8.45645521,  9.72099652,  2.26884486, -0.0982231 ,
        4.68467616, -5.87891132, -2.23202573, -4.54597516, -3.63166501,
       -6.47260448, -0.25494149, -9.34894223, -8.18096018,  4.67172684,
       -8.53862097, -5.52045076,  5.20051333,  6.43593356,  4.62656807,
       -0.78407774,  6.01525013, -9.8470548 ,  7.26315861, -0.81187932,
        0.90287167,  9.91582383, -9.68414692,  9.22632792, -7.74027283,
       -6.98351939, -9.39248254, -2.40637618,  1.88386073, -3.49959818,
       -8.92086711,  7.95984206, -4.01164455, -4.12007986,  1.88949362,
       -1.23832451,  9.94042174, -6.82585697,  9.79088727, -2.22454134,
       -8.31042533,  9.73358223, -9.42715423, -1.54921157,  3.00850872,
       -0.42704372,  9.00754799, -8.35957297,  4.20130084, -3.22674587,
       -9.59795248,  7.48184945, -5.61396974, -8.72022444, -4.26686049,
       -7.16399655, -9.60725157, -3.55253645, -0.37204589, -6.65636015,
       -7.16280586,  7.50258432, -8.96109707,  9.28642911, -0.29788887,
       -9.73481531, -3.56345775,  9.43754118,  5.10305566,  5.31997424,
        1.02299777, -3.85717717, -4.85692981,  5.00369753,  5.66379488,
        3.49247411,  7.18543683,  1.51945616,  1.95212321, -4.78848353,
       -0.59976358,  7.75857548, -6.37251227, -9.49685289, -4.98691331,
        9.03410806,  6.06671711, -0.98902141,  2.27512478,  6.27816686,
        8.7714181 , -4.61080951, -0.05131483, -5.73585186,  2.03347027,
        7.07593177, -2.06363223, -5.54346859,  1.23224762, -9.16235296,
       -7.41316765, -9.18017826,  4.385383  ,  9.89648275,  1.97327256,
       -8.25725171,  2.61605802,  3.48777179, -8.77723651, -0.48742548,
        1.11047202,  3.71478794,  2.85155998,  7.77424114, -2.39454872,
       -5.8348042 , -3.26596297, -1.07160637,  6.03360473,  9.93737948,
        4.52967536,  1.2594118 ,  1.12623349, -5.23668859,  3.13273613,
        0.62716865, -8.88213693, -7.75189218,  3.97271066, -5.72325941,
        7.41189405, -9.688778  ,  1.43220238,  7.99778626,  1.16323832,
        6.70096975, -4.98809668, -3.96875764,  4.52763073, -2.19692082,
       -6.51315927,  0.47575634,  8.80728611,  9.53243149, -6.35564752,
       -3.70782232, -5.77102195, -8.94275399,  4.69241809,  9.59563031,
        4.62906527, -4.13372871,  4.39579834, -8.37964476,  6.12633561,
        3.44571203, -8.54000157, -9.07268842,  8.51683636, -8.3009988 ,
        1.40433244, -4.02079645,  7.38670715,  2.01014087,  8.0853672 ,
       -9.70807032, -6.20103038,  0.98709881,  0.22348996, -6.542874  ,
        7.15010407,  5.2098732 ,  0.62136102,  6.65297283,  7.22624023,
       -5.14404408, -1.27071169, -9.98193563, -5.02051074, -8.77846353,
       -9.87617207, -2.39882994,  4.25520725,  2.16942571, -9.976912  ,
        2.39362798,  5.56113025,  2.63238948,  0.82817681,  9.31784216,
       -5.63404241,  1.79536128,  7.09970921, -5.27104842,  0.01695788,
       -3.78974731,  8.68848185, -2.50550906, -1.6524935 ,  5.25003909,
       -6.6015042 , -8.95915767,  8.68997782,  1.25577181, -5.21506271,
        7.58362148,  6.1168738 ,  2.93946819,  5.54235226,  0.78601974,
       -1.6169377 , -1.56918951, -8.01141256, -5.70559227, -9.96447934,
       -1.15385332, -6.14971336, -0.77118078, -5.27148622, -8.42865193,
        5.42357309, -3.26648765,  3.66936431, -1.8711219 , -1.7541717 ,
        4.16565588, -1.24497552, -3.58080512, -5.40819661,  4.91980434,
        4.72247028,  8.07507777,  9.34911837,  0.26280899,  6.18848424,
        0.81375428, -1.65969104, -7.90945737,  8.08444385, -5.86687503,
        3.61177173,  7.26484781, -6.69086842,  2.85614699,  7.75808275,
       -6.96379838, -7.43913489,  8.46173607,  7.57401974, -5.16354199,
       -0.21978289, -7.51698223,  5.78612437,  1.6520984 ,  3.55194789,
       -9.29715517,  4.08967743, -8.46740488, -1.52140939, -8.79444499,
        6.97370131,  0.93341097,  8.66684832,  0.75845129, -2.45499999,
        7.95850145,  6.82967864,  9.80521605,  7.57527447,  3.3588022 ,
        7.65485747, -7.88194897,  5.26679499,  3.270491  ,  3.34412497,
        5.76988346, -2.06374413,  2.41119655, -9.24282266,  4.66896653,
       -7.25497653,  2.78291688, -8.56363261, -5.14720543,  2.86286347,
        3.29993429,  2.78374563,  1.59095638, -0.24517971,  1.71379722,
       -4.79988753,  4.67744209,  9.80795379, -4.24250067, -3.08291519,
       -4.21998488,  9.64510694,  9.50623212, -4.38753524,  9.91233487,
        0.24335963, -2.25035282, -4.65012   , -9.24754227,  2.55731572,
        6.89521571, -9.05571189, -0.04509244,  1.12375144, -0.32428442,
        1.17768599, -4.82265295, -2.41346159,  8.00586876,  5.45318506,
       -7.70104513,  1.20528019, -6.46640138,  6.8612343 ,  3.23923029,
        9.68965017,  9.48327342, -6.29542321,  4.25917666,  8.95651101,
       -3.57965631,  7.3253664 , -2.97283296,  0.42133531, -1.68264179,
        4.18887099,  3.04655226,  4.83409707,  2.85393367, -7.52212235,
       -8.21260919, -3.79561797, -9.10784288, -0.76627198, -5.42310021,
        6.85214492, -5.59140169,  8.05884123,  7.67041756, -7.53168211,
        9.25635842, -8.14901525,  1.95444385, -3.59412474,  5.56936179,
        9.29734373, -7.82515595, -9.12079581,  0.52020073, -3.32617247,
        2.13134839,  1.47765509, -7.17415722, -4.65431335,  2.40957074,
       -1.71466282,  3.31611758,  6.84589679, -4.95778455, -8.81882907,
       -8.44028501,  7.59260766,  5.62975813, -1.57878406,  8.3379977 ,
        5.70754505,  7.63001182, -2.2428547 , -8.22142194,  6.61964244,
       -5.77580001, -8.10281005,  6.68016489,  1.49432004, -4.1173087 ,
        4.04600615,  5.92417868, -4.06331621,  8.4295584 , -4.64989644,
        3.14069978,  5.79342263,  3.98383318, -8.38603297,  3.03757238,
       -4.15871091, -3.88445101, -9.39957943, -5.33555828,  6.42769736,
        0.35765595, -8.75397886,  2.26465633,  8.66504315, -7.90989111,
       -7.8926672 , -3.48871167,  3.8270089 ,  1.50723931,  2.9487293 ,
       -0.03836586,  8.56930153,  5.98874231,  5.77467578, -7.93974002,
        7.73810533,  2.08427808, -4.43968033,  1.63619551,  4.04357778,
        4.55401134,  1.60789415,  0.58684086,  5.11111014, -2.53853815,
       -3.23488225, -4.40423113, -3.23594091, -2.50581935, -5.15138874,
       -5.24801484, -9.1297547 , -5.245082  , -8.9086799 , -5.29500699,
       -2.95443143,  8.73319761,  9.22541665, -4.20879158, -9.20482458,
       -9.8551247 , -4.85275013,  1.27561869,  7.22478798,  2.51081284])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([-1.59029677e+00,  1.21635766e+00,  1.43889435e+00,  9.69731435e-01,
       -1.64704723e+00, -1.54587973e+00, -1.11289130e+00,  3.56055541e-01,
        1.13706249e+00,  1.58046972e+00, -2.74849261e-01,  1.44556502e+00,
        1.40992857e-01, -6.41757388e-02,  1.68181518e-02,  8.33212286e-01,
        9.17540621e-01, -1.34326428e+00,  8.78840839e-01,  1.37586600e+00,
        1.43121397e+00, -4.94930079e-01,  2.77268606e-01,  8.43684798e-01,
        9.77012882e-01,  5.50412101e-01,  3.33675706e-01, -1.16873292e-01,
        7.36617402e-01,  2.44902095e-01, -2.64537241e-02,  9.77252120e-01,
       -1.45146551e+00, -1.46859267e+00, -1.37000863e+00,  5.91444867e-01,
        7.55187657e-01,  9.98253170e-01,  1.30381058e+00,  1.37528997e+00,
        1.37296506e-01, -1.44944461e+00,  1.27696085e+00,  1.62201068e+00,
       -1.35593479e-02,  1.43697259e+00, -5.87013171e-01,  7.99509364e-01,
        1.53515405e+00,  2.55539526e-01, -9.38042864e-01,  8.46319810e-01,
        1.37048261e+00, -1.18589274e-01, -4.86998791e-01,  3.18047001e-01,
       -1.10112504e+00, -7.34935894e-01, -7.37896654e-01, -6.78179951e-01,
       -2.67615200e-01,  1.18895465e+00,  1.52168196e+00, -3.87586569e-01,
        1.15049416e+00, -1.04582591e+00, -1.18541636e+00, -5.07456259e-01,
        1.11272108e+00, -1.77045633e-01,  1.02355320e+00, -1.25947511e+00,
       -5.49154883e-01, -1.27634923e+00, -1.19607093e+00, -1.19796072e+00,
       -9.77095322e-01,  1.16534822e+00, -1.05608060e-01, -4.11074901e-01,
       -1.21454495e+00, -7.68200784e-01, -1.49733844e+00, -2.64681198e-01,
       -2.87917354e-02, -5.58619935e-01,  1.64580070e+00, -6.28740535e-01,
        1.40035713e+00, -1.30333050e+00,  8.64895489e-01, -1.15983961e+00,
       -1.51812960e+00, -1.18606129e+00, -1.06432018e+00, -1.51354190e+00,
       -1.20617512e-02, -5.52181213e-01, -9.97822505e-01, -1.56569047e+00,
       -6.23132099e-01,  1.44435536e+00,  2.48598067e-01,  6.63832170e-01,
        3.83926410e-01, -1.55107782e-01, -1.65286252e+00,  1.03764036e-01,
        5.74970759e-01,  1.49315465e+00,  1.50877353e+00,  4.09275921e-02,
        8.68694261e-01,  8.11068077e-01,  1.40032210e+00, -6.22845061e-01,
        9.24170981e-01,  1.49112486e+00,  5.36624418e-01,  7.72206208e-01,
       -9.61122134e-01, -1.93544360e-01, -1.52349170e+00,  1.07865340e+00,
        2.54994470e-01,  3.40762756e-01, -2.27588126e-01, -1.44067792e+00,
       -1.05238419e-01, -6.41886987e-01, -8.88388496e-01, -1.13489655e+00,
        9.98308507e-01, -1.03808720e+00, -1.29526446e-01, -3.63547813e-01,
        6.66612598e-01,  1.57432501e+00,  1.14241418e+00,  1.61091359e-01,
        4.34281238e-01, -1.52040211e+00,  3.19848091e-01, -1.42636227e+00,
        5.14329841e-01, -2.16451554e-01,  9.63070159e-01,  4.02300698e-01,
        1.52936168e+00,  1.54047139e-01,  1.01641696e+00,  1.54086628e+00,
       -1.15808321e+00,  9.48215995e-01,  7.44966854e-01,  1.54436660e+00,
        5.09186382e-01,  2.05557562e-01, -1.57193178e+00, -3.67141210e-01,
        4.43339344e-01, -1.06068143e+00, -1.41781591e+00, -5.64331663e-01,
        1.55081522e+00,  1.44242250e+00, -1.38334514e+00,  7.73654223e-01,
        8.14931414e-01, -2.31257150e-01,  1.45662549e+00,  4.97244765e-01,
       -6.74302668e-01, -1.13805713e+00, -1.57745967e+00, -1.42273076e+00,
       -1.38813931e+00, -1.33409753e+00,  1.47559609e+00,  1.13463613e+00,
        3.47401265e-01,  6.77822843e-02,  1.19027076e+00,  2.46922845e-01,
        9.05808396e-01,  1.60491122e+00, -8.71928242e-01,  1.46361972e+00,
        1.50779261e+00,  1.13190359e-01, -9.85492228e-01,  2.23413014e-01,
       -1.55229067e+00,  1.11419301e+00, -1.34776405e+00, -3.01422327e-01,
       -2.14337086e-01, -1.27082895e+00,  1.40554012e+00,  1.53310075e+00,
       -1.16598118e+00,  1.53161615e+00, -2.46919922e-01, -1.22962515e+00,
        1.30876240e-01, -5.49745815e-01, -1.29329469e+00, -1.39323855e+00,
       -8.24140884e-01, -3.44807457e-02,  9.38005544e-01,  8.79643173e-02,
       -8.51379855e-03,  1.60708329e+00, -6.26057503e-01,  1.49962749e+00,
       -1.47307364e+00,  2.03299915e-01,  1.43905189e+00,  3.55009069e-01,
        1.12756818e+00,  1.44360059e+00, -1.11324734e+00,  1.38568396e+00,
        2.82051101e-01,  1.46819022e+00, -3.21355100e-01,  5.45698219e-01,
       -5.85196759e-01, -6.15778466e-01,  1.57108780e+00, -1.22341011e+00,
       -6.68260103e-02,  1.27783110e+00,  1.58416548e+00,  1.29550645e+00,
       -1.15680580e+00, -4.70378053e-01, -1.23153265e+00,  9.55655055e-01,
       -1.41565750e+00,  4.24228538e-01,  7.37414144e-01, -4.79425097e-01,
        1.35508300e+00,  4.84393506e-01, -1.31482079e+00, -3.72710331e-01,
       -1.50754539e+00, -1.33822724e+00,  1.52458009e+00,  1.39016029e+00,
        9.13703550e-01,  1.40045588e+00, -6.57637021e-01, -1.55240359e+00,
       -9.84773829e-01, -1.44003330e+00, -2.70991223e-01, -3.24770960e-02,
        1.42723743e+00,  1.07362720e+00, -8.02987409e-01,  1.27824109e-01,
       -5.03013444e-01, -1.57662574e+00,  1.14033078e+00,  1.07690863e+00,
       -1.05165157e+00, -8.88996094e-01, -1.52951078e-01, -1.27530508e+00,
       -1.59836645e+00, -1.01789575e+00,  1.15250842e-01, -1.42757537e+00,
       -1.49044618e+00, -1.32758137e+00, -3.67639146e-01,  1.32059283e+00,
       -6.58199747e-01,  1.17975264e-01,  1.42349599e+00, -1.31332958e-02,
        8.93204614e-01, -1.12089846e+00, -1.18315748e+00,  2.71548098e-01,
        1.55256239e+00, -2.20886942e-01, -9.07903962e-01, -1.36873395e+00,
       -4.02892021e-01,  9.24603483e-03, -1.75587004e+00,  4.25341535e-01,
        7.47469991e-01,  9.18210018e-01, -9.78082740e-01,  1.41946600e+00,
       -1.56269876e+00,  1.36868739e+00, -1.48612346e+00, -1.47290445e+00,
        1.22187585e+00,  1.39750834e+00,  1.52038954e+00,  8.70997106e-01,
        1.34654692e+00, -6.88344930e-01,  1.48638841e+00,  1.16940684e+00,
        6.26014562e-02,  1.52132931e+00, -1.59526688e+00, -1.16416299e+00,
       -8.50644396e-01, -5.55268164e-01,  1.47308834e+00, -9.52377564e-01,
        1.45398736e+00,  4.10808789e-01,  6.03486444e-02, -5.16761096e-02,
        1.33467638e+00,  1.12604548e+00, -6.70779596e-01,  1.21815171e+00,
        1.08660213e+00,  1.37824923e+00, -1.61747662e-01, -2.01818111e-01,
       -1.04128626e+00,  3.60977547e-01, -4.32281359e-01,  1.53253243e+00,
       -1.22937282e+00, -7.24185469e-01,  9.42381283e-01,  4.27299060e-01,
        1.42300097e+00,  3.78555588e-01, -5.39649091e-01, -1.00357909e+00,
        1.54783563e+00,  2.49137297e-01,  3.85311096e-01,  1.46353564e+00,
        4.70527237e-01, -2.00597079e-01,  3.83981412e-01, -1.49070577e+00,
       -9.85860177e-01, -7.45684833e-01,  4.30307793e-01, -5.92021356e-01,
       -9.05392226e-01,  7.34754901e-01, -9.69957426e-01, -5.71589044e-01,
        9.27701964e-01, -6.91701515e-01,  1.27777789e+00,  3.76438660e-01,
        1.37192892e+00,  1.33463327e+00, -1.52859026e+00,  6.24895971e-01,
       -1.40257776e+00,  1.41286251e+00,  2.79964205e-01, -7.95073045e-01,
       -9.98402319e-01,  3.06734745e-01,  1.09954632e+00, -1.22726200e+00,
        9.62173084e-01, -3.16100873e-01, -1.39099499e+00, -1.50174558e+00,
        9.79130540e-01, -8.27911983e-01,  1.06577375e+00,  1.19748851e+00,
        1.71301999e-01,  1.30958021e+00, -1.33254349e+00, -6.44191092e-01,
        6.66225060e-01,  7.74683460e-01, -1.37077893e+00,  1.07613542e+00,
       -9.55415044e-01, -3.13286024e-02,  1.01267976e+00, -1.41605178e+00,
        7.67085921e-01, -6.42872183e-01, -5.62858894e-01,  1.06938586e+00,
       -1.37898273e+00, -1.03106482e-01,  7.10173394e-01,  1.41796802e+00,
        6.49666433e-01, -1.16509375e-01, -1.59418364e+00,  7.60728134e-01,
       -9.81906973e-01, -1.26206506e+00,  1.60298989e+00,  1.32614747e+00,
        1.73120599e+00, -4.45050606e-02,  5.28903061e-01,  1.19813208e+00,
        1.14023144e+00, -9.73064114e-01,  1.30973762e+00,  1.03868551e-01,
        6.78866458e-01, -1.28026297e+00,  1.91380584e-01,  8.16092631e-01,
       -1.14959023e+00, -1.59209167e+00, -5.71357981e-01, -9.38583975e-01,
        1.33944276e+00,  1.20432607e+00, -1.45374689e+00,  5.77531329e-01,
        5.59869927e-01, -6.66987461e-01,  3.39480037e-01,  2.05694199e-01,
       -1.41940842e+00,  2.14452384e-01,  1.19447276e+00,  2.79361374e-01,
       -7.24239462e-01, -1.41641489e+00,  4.34799826e-01, -1.34068578e+00,
       -1.41949261e+00, -8.56821350e-01,  9.27898114e-01, -9.51880961e-01,
       -1.28894968e+00, -1.06467742e+00, -9.33442979e-01,  1.29056173e+00,
        1.29664300e+00,  4.39703969e-01,  4.87786665e-02, -1.07828797e+00,
        1.03572420e+00,  1.04977245e+00, -1.53549230e+00,  1.44645659e+00,
        1.47739398e+00,  2.78295415e-01,  2.21939427e-01, -1.09907263e+00,
        3.04737859e-01,  1.49985676e+00,  1.15521634e+00, -9.81907376e-01,
        1.43393295e+00,  1.37127543e+00,  1.05615815e+00, -2.34985557e-01,
        1.09093243e+00,  1.38630447e+00,  4.79119536e-01, -1.49366768e+00,
        1.35111100e+00, -2.44142066e-01,  1.53755814e+00,  7.22346716e-01,
       -1.52883978e+00,  1.19456394e+00, -9.34773795e-01, -5.17476584e-01,
        1.18040651e+00, -1.18948257e+00, -9.97857701e-01,  1.28688474e+00,
       -2.25478004e-01,  4.79822137e-01,  1.02799324e+00, -3.47418573e-01,
        9.98280751e-01, -1.11487292e+00, -1.15851439e+00, -1.96774246e-01,
       -1.14971042e+00, -6.13465199e-01,  1.43466203e+00,  8.58994140e-02,
        2.36707352e-03, -1.50940227e+00, -5.35730871e-01,  7.35793797e-01,
       -1.56475972e+00, -5.52711527e-02, -7.50123076e-01,  7.57466378e-01,
        7.20289772e-02, -1.18230259e+00, -9.06131338e-01,  1.09200060e+00,
        1.08323120e+00, -1.39669763e+00, -1.48508184e+00, -1.44680436e+00,
       -5.93997536e-01, -1.32545462e+00,  7.79721086e-01, -1.20969201e+00,
       -1.10923582e+00, -5.11836678e-01, -9.71781186e-01, -1.09904430e+00,
        3.03561674e-01, -3.89327478e-01,  1.50841517e+00,  1.44188999e+00,
        1.09430754e-01,  1.35409905e+00, -8.88367361e-01,  1.42889784e+00,
       -8.36471230e-01, -7.37088109e-01, -1.10043013e+00,  4.64659834e-04,
       -6.44411023e-01,  3.21166687e-01,  1.33819623e+00, -1.56319138e+00,
       -9.82398735e-01, -8.42375159e-01, -5.87797826e-02,  1.41268475e+00,
       -1.38800288e+00,  1.35240162e+00, -1.43946392e+00, -1.39902680e+00,
       -3.85496916e-01,  5.13265176e-01, -1.49850333e+00,  1.36894585e+00,
       -5.04357247e-01, -1.55449320e+00, -1.42174907e+00,  1.62714714e+00,
        1.44184947e+00, -7.14264450e-02, -8.34816054e-01, -4.34680855e-02,
        1.64145977e+00, -2.76319398e-01,  3.13679057e-01, -1.10543788e+00,
        8.39693667e-01,  9.97293972e-01, -5.80483052e-01, -1.23787547e+00,
       -5.73076238e-01,  1.44262483e+00, -1.19352237e+00, -7.32893797e-01,
        1.22992904e+00, -1.05600727e+00,  1.33240569e+00, -1.35410109e+00,
       -5.81195959e-01,  9.79626463e-02, -1.13091590e+00, -1.31887404e+00,
        1.58291030e+00, -2.61991162e-01, -5.75471854e-01,  1.24074548e+00,
        4.70140654e-01,  1.48948321e+00, -9.87918415e-01,  8.58844405e-01,
       -2.99151579e-02,  1.10664874e+00, -1.77684010e+00,  2.06189788e-01,
        2.52242879e-02, -3.41890962e-01,  1.06519877e-01, -1.47010889e+00,
       -4.49257800e-01,  1.52053181e+00, -2.75554263e-01,  4.85439263e-01,
       -9.39748681e-01,  1.05448753e-01, -1.15342090e+00, -1.08420694e+00,
       -1.48544316e+00, -8.09764226e-01,  1.47445455e+00, -8.66044206e-01,
        7.61350498e-01,  1.63266090e+00, -5.82211466e-01, -5.78635596e-01,
       -8.34398281e-01, -1.50001311e+00, -5.15757658e-01,  1.40582701e+00,
       -6.81312508e-01,  9.39467557e-01, -5.63814637e-01,  1.44514827e+00,
        1.30974358e+00, -7.15031044e-01, -1.54080235e+00,  9.59090095e-01,
       -4.63586966e-01, -3.53496194e-01,  9.83931807e-01,  6.51284151e-01,
        1.32047712e+00,  1.07434990e+00,  1.38072283e+00,  4.02876788e-01,
       -1.43992688e+00,  1.29398634e+00,  3.47768212e-01, -8.13447082e-01,
       -1.26333895e+00,  7.83222613e-01, -1.80596341e-01, -1.48084865e+00,
       -1.14610906e+00,  1.35478228e-02,  1.02551416e+00, -8.95164351e-01,
       -1.47520751e+00,  1.36317857e+00, -5.62734425e-01, -1.55924880e+00,
       -8.66984544e-01, -1.05562166e+00,  1.48234523e+00, -1.30521556e+00,
        7.71126863e-02, -4.82131909e-01, -3.54193343e-01, -1.32006245e+00,
        8.51661316e-02, -1.06951895e+00,  1.28987583e+00, -1.51822681e+00,
        1.01636440e+00, -1.22676580e+00,  1.20763283e+00, -5.89049159e-01,
       -1.64583757e-01, -8.04704936e-01, -1.25569329e+00, -6.89897220e-01,
       -1.40273943e+00, -1.10150609e+00,  1.53633402e+00, -3.65023231e-01,
       -1.58771635e+00, -9.98474083e-03,  8.44037967e-01, -2.67186544e-01,
        1.37329283e+00, -1.47667973e+00, -6.22830147e-01, -3.37704308e-01,
        1.28346251e+00,  1.26130337e+00,  1.25886434e+00, -1.40652540e+00,
       -1.43688125e+00,  1.28130842e+00, -1.33996336e+00,  2.79062247e-01,
        3.90563875e-01, -6.54074887e-01, -8.84694277e-01, -4.72359252e-01,
        9.07759211e-01,  4.22511666e-01,  1.35387231e+00, -1.43362973e+00,
       -1.48958954e+00,  9.60499826e-02,  1.27375302e+00,  5.39154415e-01,
        1.12166737e+00,  1.13327547e+00,  1.44425590e+00,  5.60747121e-01,
        9.85572496e-01,  7.33229154e-01, -4.88995306e-01, -1.38273161e+00,
        1.44065916e+00,  4.00612567e-01,  1.51023909e+00, -4.05057901e-01,
        1.16206673e+00, -9.42158861e-01, -6.41457955e-01, -1.12877691e+00,
       -1.31448701e+00, -1.65975164e+00,  1.28958766e+00, -7.65840853e-01,
        1.07813505e+00, -1.53382908e-01, -7.62744008e-01, -2.43457581e-01,
        8.42334238e-01, -2.61501035e-01,  4.70483034e-01,  1.45127053e+00,
        6.97037213e-01,  1.36419209e+00, -9.92495361e-01,  1.29165107e+00,
        1.31065512e+00, -1.38574603e+00, -1.21370670e+00, -1.26135652e+00,
       -1.25718836e+00,  3.25891372e-01, -1.54544429e+00, -1.38212359e+00,
        1.44406977e-01,  8.18452247e-01,  9.29602795e-01, -6.54197165e-01,
        1.90071583e-01, -1.12467703e+00,  4.56827337e-01, -1.39623605e+00,
        1.39973173e+00,  1.00143407e+00, -7.90029483e-01,  6.67625543e-02,
       -1.30513021e+00, -1.33370204e+00,  2.10806007e-02,  3.90566098e-01,
        1.57098426e+00,  1.12788228e+00, -8.13403734e-01,  1.28125017e+00,
        1.37592442e+00, -1.53164533e+00,  1.42658773e+00,  1.38008602e+00,
       -1.48550911e+00, -5.90247877e-01, -7.90028714e-01, -1.49949698e+00,
        9.15993137e-01,  1.47333288e+00, -9.26212547e-01, -1.00725226e+00,
       -2.02449152e-01, -5.51116442e-01,  1.90594807e-01, -1.59392914e+00,
        3.95058224e-01, -1.32553724e+00, -1.37149436e+00, -1.13796908e+00,
        1.01544658e+00,  1.22063509e+00, -3.19198097e-01, -3.10978187e-01,
       -7.16691002e-01,  6.45171498e-01, -1.53940967e+00,  9.62151551e-01,
       -1.25321885e+00,  1.36299691e+00,  1.60060177e+00,  4.08259584e-01,
        1.06776637e+00,  5.89430852e-01, -7.90386634e-01,  1.75077869e+00,
       -8.00657422e-01, -1.84936920e-02,  5.32087414e-01, -2.87576895e-01,
       -6.46022950e-01, -1.09647889e+00,  1.46930503e+00, -1.52935697e+00,
        1.85075884e-01,  1.51185954e+00,  1.35973540e+00,  1.18854622e+00,
       -5.22202515e-01, -1.39323304e+00, -4.15561015e-01,  8.77978340e-01,
        3.86777533e-02,  4.96356650e-01, -8.63409883e-01, -1.16464390e+00,
        6.19260064e-01, -1.17070036e+00, -1.03345004e+00, -1.38315205e+00,
       -1.55683810e+00,  1.56236677e+00,  1.35882122e+00,  1.39106279e+00,
        9.94120278e-01,  4.80540804e-01, -1.58657988e+00, -5.09433191e-02,
       -4.65879647e-01, -1.06430532e+00, -3.69378094e-01,  8.58977858e-02,
        1.23675266e+00, -1.00420524e+00, -1.05963785e+00,  2.73923805e-02,
        5.77930729e-02, -1.17563922e+00, -1.28080930e+00,  1.38324192e+00,
        1.18402942e+00, -1.47649700e+00, -1.35207884e+00, -1.46525925e+00,
       -1.65934302e+00, -1.36696475e+00, -1.41689267e+00,  6.78172505e-01,
       -4.08910826e-01,  5.02397879e-01, -1.47774610e+00,  1.54521368e+00,
       -2.27610001e-01, -6.61926321e-01,  1.53181784e+00, -5.73900179e-01,
        2.22961875e-01,  5.82668977e-01, -7.61201973e-01, -2.70697154e-01,
        7.39709568e-01,  5.65105411e-01, -1.19016320e+00, -1.30700887e+00,
       -1.21736174e+00, -1.45540650e+00, -1.48089758e+00,  5.90130032e-02,
        1.64295677e+00, -4.93535359e-01,  1.29504891e+00, -1.47996750e+00,
        9.71700837e-01,  3.55853286e-01,  8.54289559e-01,  5.83698090e-01,
        1.48399656e+00,  1.13499520e+00, -1.34573347e+00, -1.36455557e+00,
        9.03683808e-02,  4.08107718e-01,  6.73343229e-01,  9.54460890e-01,
        1.52004224e+00,  9.99216344e-01, -1.09167983e+00,  1.64607283e+00,
        8.55647394e-01, -8.88999120e-01,  6.44329943e-01, -1.53505763e+00,
        1.51779032e+00, -1.61602177e+00,  6.08086748e-01, -6.13249759e-01,
        4.99110011e-01, -1.48032318e+00, -1.15738466e+00, -1.23262044e+00,
       -1.60968788e+00, -9.31634706e-01,  6.17454154e-01, -2.75416475e-01,
        8.50329049e-01,  9.36658851e-01, -4.70589170e-01, -1.47244457e-02,
        9.03912763e-01,  7.13660971e-01,  8.77788145e-01,  3.27142108e-02,
       -1.45477315e+00,  1.31824374e+00,  1.27905270e+00, -4.39586026e-01,
        8.72543353e-01,  1.29555660e+00, -1.24439537e+00, -1.12764627e+00,
       -9.00031712e-01, -1.32031899e+00, -1.61625759e+00, -1.60824063e+00,
       -1.56662748e+00, -9.86476043e-01, -4.39632271e-01,  5.33628979e-01,
       -1.03941085e+00,  1.33646372e+00,  1.78551259e-01, -2.88236286e-01,
        4.21159676e-01, -6.30356841e-01, -1.42324926e+00, -4.12326911e-01,
       -5.01855701e-01, -1.36744835e+00,  1.07692506e+00, -2.71953650e-01,
        4.54802153e-01, -3.17370337e-01, -2.84135852e-02,  1.31788633e+00,
       -1.11656924e+00, -1.28070769e+00,  9.28277261e-02,  2.79852824e-01,
       -1.03884512e+00, -1.41930061e+00, -5.23565989e-01,  2.89077571e-01,
       -1.23214031e+00, -1.39427070e+00, -1.33478868e+00,  9.62212130e-01,
       -1.44592304e+00, -9.15339416e-01,  1.37308482e+00,  1.75274378e-02,
        1.14844349e-01,  9.67386168e-01, -5.31589126e-02,  9.85463879e-01,
       -1.39172371e+00,  7.53470963e-02,  1.37239085e+00, -4.88384559e-01,
        9.60112843e-02,  4.53829855e-02, -1.58174481e-01, -3.72024869e-01,
       -1.07065055e+00,  6.15513259e-01,  4.10037100e-01,  1.39420914e+00,
        6.41816347e-01,  1.31629014e+00,  1.17558419e+00,  1.12364518e+00,
        1.38924208e+00, -9.70789434e-01,  1.27950174e+00,  1.07016639e+00,
       -1.43890290e+00, -1.19064169e+00, -1.28016341e+00, -1.50100222e+00,
       -1.49054660e+00, -1.17517891e+00,  1.46251532e+00,  1.56273612e+00,
        1.13611532e+00, -4.23621607e-01, -1.32334902e+00, -9.69125323e-02,
       -1.39389209e+00,  1.17516626e+00, -9.83092310e-01, -1.10209384e+00])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># # 2번째 방법
# def f(x,a,b):
#     return np.sin(a*x+b)
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># xs = np.random.uniform(xmin,xmax,n)
# ys = f(xs,a,b) + np.random.normal(0,1) #노이즈 추가
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ys
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">ys</span><span class="p">,</span><span class="s">'.'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x1dc0e1da940&gt;]
</code></pre></div></div>

<p><img src="./img/machinelearning/10/output_16_1.png" alt="output_16_1" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1차함수 
# bestA, bestB = None, None
# bestLoss = 1e9 # 10의 9승
# t0 = time.time()
# lossesBySearch = []
</span>
<span class="c1"># #numpy 모듈의 arange 함수는 반열린구간 [start, stop) 에서 step 의 크기만큼 일정하게 떨어져 있는 숫자들을 array 형태로 반환해 주는 함수다.
</span>
<span class="c1"># for aa in np.arange(-10,10,0.1):  
#     for bb in np.arange(-10,10,0.1):
#         yys = f(xs, aa, bb)
#         loss = ((yys - ys) ** 2).mean()  #sum() 대신에 mean()으로 대체
#         lossesBySearch.append(loss)
</span>        
<span class="c1">#         if loss &lt; bestLoss:
#             bestA, bestB = aa, bb
#         bestLoss = loss
# tf = time.time()
</span>
<span class="c1"># print("Truth:",a,b)
# print("Fit results:", bestA,bestB)
# print("Elapsed time:", (tf-t0))
</span>
<span class="c1"># 2차함수 
</span><span class="n">bestA</span><span class="p">,</span> <span class="n">bestB</span><span class="p">,</span> <span class="n">bestC</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
<span class="n">bestLoss</span> <span class="o">=</span> <span class="mf">1e9</span> <span class="c1"># 10의 9승
</span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">lossesBySearch</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#numpy 모듈의 arange 함수는 반열린구간 [start, stop) 에서 step 의 크기만큼 일정하게 떨어져 있는 숫자들을 array 형태로 반환해 주는 함수다.
</span>
<span class="k">for</span> <span class="n">aa</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">):</span>  
    <span class="k">for</span> <span class="n">bb</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">cc</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.01</span><span class="p">):</span>
            <span class="n">yys</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">aa</span><span class="p">,</span> <span class="n">bb</span><span class="p">,</span> <span class="n">cc</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">yys</span> <span class="o">-</span> <span class="n">ys</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>  <span class="c1">#sum() 대신에 mean()으로 대체
</span>            <span class="n">lossesBySearch</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestLoss</span><span class="p">:</span>
                <span class="n">bestA</span><span class="p">,</span> <span class="n">bestB</span><span class="p">,</span> <span class="n">bestC</span> <span class="o">=</span> <span class="n">aa</span><span class="p">,</span> <span class="n">bb</span><span class="p">,</span> <span class="n">cc</span>
            <span class="n">bestLoss</span> <span class="o">=</span> <span class="n">loss</span>
<span class="n">tf</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Truth:"</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Fit results:"</span><span class="p">,</span> <span class="n">bestA</span><span class="p">,</span><span class="n">bestB</span><span class="p">,</span><span class="n">bestC</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Elapsed time:"</span><span class="p">,</span> <span class="p">(</span><span class="n">tf</span><span class="o">-</span><span class="n">t0</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Truth: 1.5 1.6 0.1
Fit results: 1.9000000000000001 1.9000000000000001 0.72
Elapsed time: 0.8331818580627441
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lossesBySearch</span><span class="p">)</span>  
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x1dc0e3dc710&gt;]
</code></pre></div></div>

<p><img src="./img/machinelearning/10/output_18_1.png" alt="output_18_1" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># #1방법
# plt.plot(xs,ys,'.')
# plt.plot([xmin,xmax], [f(xmin,a,b), f(xmax,a,b)])
# xxs = np.linspace(xmin,xmax,100)
# yys = f(xxs,bestA,bestB)
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#2방법
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">ys</span><span class="p">,</span><span class="s">'.'</span><span class="p">)</span>
<span class="n">xxs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">yBest</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span> <span class="n">bestA</span><span class="p">,</span> <span class="n">bestB</span><span class="p">,</span><span class="n">bestC</span><span class="p">)</span>
<span class="n">yTrue</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span> <span class="n">yTrue</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span> <span class="n">yBest</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x1dc0e4279b0&gt;]
</code></pre></div></div>

<p><img src="./img/machinelearning/10/output_20_1.png" alt="output_20_1" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1차함수
# def gradF(x,a,b):  #gradient 계산
#     return (x,1)
</span>
<span class="c1"># 2차함수
</span><span class="k">def</span> <span class="nf">gradF</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">):</span>  <span class="c1">#gradient 계산
</span>    <span class="c1">## y = a*x*x + b*x +c
#     return (x*x,x,1)
</span>    <span class="n">da</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="n">x</span>
    <span class="n">dc</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">da</span><span class="p">,</span><span class="n">db</span><span class="p">,</span> <span class="n">dc</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># #1차함수
# t0 = time.time()
# fitA, fitB = -10,-10
# eta = 1e-4 #parameter 파라미터 
# lossesByGrad = []
</span>

<span class="c1"># for i in range(100000):
#     yys = f(xs,fitA,fitB)
#     gradA , gradB = gradF(xs, fitA, fitB)
#     fitA = fitA - eta*((yys-ys)*gradA).mean() # sum()대신에 mean()으로 대체
#     fitB = fitB - eta*((yys-ys)*gradB).mean()
#     loss = ((yys-ys)**2).sum()
#     lossesByGrad.append(loss)
</span>
<span class="c1"># tf = time.time()
# print("Truth:",a,b)
# print("Fit results:",fitA,fitB)
# print("Elapsed time:",(tf-t0))
</span>

<span class="c1">#2차함수
</span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">fitA</span><span class="p">,</span> <span class="n">fitB</span><span class="p">,</span> <span class="n">fitC</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mi">0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">lossesByGrad</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">):</span>
    <span class="n">yys</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">fitA</span><span class="p">,</span><span class="n">fitB</span><span class="p">,</span><span class="n">fitC</span><span class="p">)</span>
    <span class="n">gradA</span> <span class="p">,</span> <span class="n">gradB</span><span class="p">,</span> <span class="n">gradC</span> <span class="o">=</span> <span class="n">gradF</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">fitA</span><span class="p">,</span> <span class="n">fitB</span><span class="p">,</span> <span class="n">fitC</span><span class="p">)</span>
    <span class="n">fitA</span> <span class="o">=</span> <span class="n">fitA</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="p">((</span><span class="n">yys</span><span class="o">-</span><span class="n">ys</span><span class="p">)</span><span class="o">*</span><span class="n">gradA</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># sum()대신에 mean()으로 대체
</span>    <span class="n">fitB</span> <span class="o">=</span> <span class="n">fitB</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="p">((</span><span class="n">yys</span><span class="o">-</span><span class="n">ys</span><span class="p">)</span><span class="o">*</span><span class="n">gradB</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">fitC</span> <span class="o">=</span> <span class="n">fitC</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="p">((</span><span class="n">yys</span><span class="o">-</span><span class="n">ys</span><span class="p">)</span><span class="o">*</span><span class="n">gradC</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">yys</span><span class="o">-</span><span class="n">ys</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">lossesByGrad</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

   <span class="c1"># if loss &lt; 1e4: break  #while true일 경우 사용함
</span>    
<span class="n">tf</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Truth:"</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Fit results:"</span><span class="p">,</span><span class="n">fitA</span><span class="p">,</span><span class="n">fitB</span><span class="p">,</span><span class="n">fitC</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Elapsed time:"</span><span class="p">,(</span><span class="n">tf</span><span class="o">-</span><span class="n">t0</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Truth: 1.5 1.6 0.1
Fit results: 1.5085357103949482 1.600510211957475 0.10446693264475382
Elapsed time: 8.28587293624878
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lossesByGrad</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[372.3115604303475,
 370.29432286808674,
 368.28559880905647,
 366.28538025208843,
 364.2936589734845,
 362.31042652781593,
 360.335674248745,
 358.36939324987077,
 356.4115744255973,
 354.4622084520297,
 352.5212857878871,
 350.58879667544386,
 348.6647311414897,
 346.7490789983153,
 344.84182984471715,
 342.9429730670281,
 341.0524978401672,
 339.17039312870935,
 337.2966476879816,
 335.43125006517545,
 333.57418860048375,
 331.72545142825425,
 329.8850264781673,
 328.0529014764271,
 326.22906394698293,
 324.4135012127608,
 322.6062003969188,
 320.8071484241159,
 319.01633202180915,
 317.23373772155975,
 315.4593518603597,
 313.6931605819791,
 311.93514983832813,
 310.1853053908351,
 308.4436128118438,
 306.7100574860269,
 304.98462461181134,
 303.26729920282776,
 301.5580660893662,
 299.8569099198536,
 298.16381516234196,
 296.4787661060148,
 294.80174686270294,
 293.132741368419,
 291.4717333849038,
 289.81870650118356,
 288.1736441351434,
 286.53652953510976,
 284.9073457814502,
 283.28607578817923,
 281.67270230457814,
 280.0672079168302,
 278.4695750496577,
 276.87978596797745,
 275.2978227785627,
 273.7236674317156,
 272.1573017229508,
 270.5987072946839,
 269.04786563793544,
 267.5047580940356,
 265.96936585634296,
 264.44166997197016,
 262.9216513435137,
 261.4092907307962,
 259.90456875260975,
 258.40746588846866,
 256.91796248037224,
 255.43603873456397,
 253.96167472330544,
 252.49485038665102,
 251.03554553422762,
 249.58373984702,
 248.13941287916003,
 246.70254405972005,
 245.2731126945073,
 243.85109796786554,
 242.43647894447813,
 241.02923457117356,
 239.62934367873243,
 238.23678498369713,
 236.85153709018388,
 235.4735784916958,
 234.10288757293807,
 232.73944261163038,
 231.3832217803249,
 230.03420314822043,
 228.69236468297726,
 227.3576842525361,
 226.03013962692899,
 224.70970848009316,
 223.39636839168793,
 222.0900968488989,
 220.790871248254,
 219.49866889742643,
 218.21346701704107,
 216.93524274247676,
 215.66397312566554,
 214.39963513689008,
 213.14220566657758,
 211.89166152708708]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">yscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lossesByGrad</span><span class="p">,</span> <span class="s">'.-'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x1dc0e4864a8&gt;]
</code></pre></div></div>

<p><img src="./img/machinelearning/10/output_24_1.png" alt="output_24_1" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1차함수
# plt.plot(xs,ys,'.')
# xxs = np.linspace(xmin,xmax,100)
# yGrad = f(xxs,fitA, fitB)
# yScan = f(xxs, bestA, bestB)
# yTrue = f(xxs,a,b)
# plt.plot(xxs, yTrue)
# plt.plot(xxs, yScan)
# plt.plot(xxs, yGrad)
</span>
<span class="c1"># 2차함수
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">ys</span><span class="p">,</span><span class="s">'.'</span><span class="p">)</span>
<span class="n">xxs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">yGrad</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span><span class="n">fitA</span><span class="p">,</span> <span class="n">fitB</span><span class="p">,</span> <span class="n">fitC</span><span class="p">)</span>
<span class="n">yScan</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span> <span class="n">bestA</span><span class="p">,</span> <span class="n">bestB</span><span class="p">,</span> <span class="n">bestC</span><span class="p">)</span>
<span class="n">yTrue</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span> <span class="n">yTrue</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span> <span class="n">yScan</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xxs</span><span class="p">,</span> <span class="n">yGrad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x1dc0e728f98&gt;]
</code></pre></div></div>

<p><img src="./img/machinelearning/10/output_25_1.png" alt="output_25_1" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://SEONGJAE-YOO.github.io/">Big Data</a> &copy; 2021</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://seongjae-yoo.github.io/" target="_blank" rel="noopener">SeongJae Yu 블로그</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="#"></a>
        <div class="subscribe-overlay-content">
            
            <h1 class="subscribe-overlay-title">Search Big Data</h1>
            <p class="subscribe-overlay-description">
                원하는 검색어를 입력하세요</p>
            <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()"
               id="searchtext" type="text" name="searchtext"
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
        </div>
    </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-xxxxxxxx-x', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
